function [grad_W, grad_b] = Backward(W, b, X, Y, act_h, act_a)
% [grad_W, grad_b] = Backward(W, b, X, Y, act_h, act_a) computes the gradient
% updates to the deep network parameters and returns them in cell arrays
% 'grad_W' and 'grad_b'. This function takes as input:
%   - 'W' and 'b' the network parameters
%   - 'X' and 'Y' the single input data sample and ground truth output vector,
%     of sizes Nx1 and Cx1 respectively
<<<<<<< HEAD
%   - 'act_a' and 'act_h' the network layer pre and post activations when forward
%     forward propogating the input sample 'X'

n = size(act_h,2);
dl_da = act_h{1,n} - Y;

for i = n:-1:2
   grad_W{1,i} = dl_da*act_h{1,i-1}';
   grad_b{1,i} = dl_da;
   
   dl_dh = W{1,i}'*dl_da;
   dh_da = act_h{1,i-1}.*(1-act_h{i-1});
   dl_da = dl_dh.*dh_da;
end

grad_W{1,1} = dl_da*X';
grad_b{1,1} = dl_da;
=======
%   - 'act_h' and 'act_a' the network layer pre and post activations when forward
%     forward propogating the input smaple 'X'


>>>>>>> 85714d4c789007d8982fbbdad2097b25af7c9a8e

end
